{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9821978,"sourceType":"datasetVersion","datasetId":6022623}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pypdf\n!pip install -q transformers einops accelerate langchain bitsandbytes\n!pip install sentence_transformers\n!pip install --upgrade llama-index\n!pip install llama-index-llms-huggingface\n!pip install -U langchain-community\n!pip install llama-index-embeddings-huggingface","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pydantic import BaseModel\n\nclass DeployedModel(BaseModel):\n    model_id: int\n    # other fields\n\n    class Config:\n        protected_namespaces = ()\n\nclass HuggingFaceLLM(BaseModel):\n    model_name: str\n    model_kwargs: dict\n    # other fields\n\n    class Config:\n        protected_namespaces = ()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T10:09:46.334166Z","iopub.execute_input":"2024-11-06T10:09:46.335454Z","iopub.status.idle":"2024-11-06T10:09:46.344835Z","shell.execute_reply.started":"2024-11-06T10:09:46.335395Z","shell.execute_reply":"2024-11-06T10:09:46.343806Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import BitsAndBytesConfig\n\n# Import modules from llama_index\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.llms.huggingface import HuggingFaceLLM\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core import PromptTemplate","metadata":{"execution":{"iopub.status.busy":"2024-11-06T10:09:49.583047Z","iopub.execute_input":"2024-11-06T10:09:49.583449Z","iopub.status.idle":"2024-11-06T10:09:49.589025Z","shell.execute_reply.started":"2024-11-06T10:09:49.583413Z","shell.execute_reply":"2024-11-06T10:09:49.588051Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Use GPU if available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-11-06T10:09:53.872864Z","iopub.execute_input":"2024-11-06T10:09:53.873272Z","iopub.status.idle":"2024-11-06T10:09:53.971457Z","shell.execute_reply.started":"2024-11-06T10:09:53.873236Z","shell.execute_reply":"2024-11-06T10:09:53.970330Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Replace the path with your actual data directory\ndocuments = SimpleDirectoryReader(\"/kaggle/input/ai-papers\").load_data()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T10:09:56.426574Z","iopub.execute_input":"2024-11-06T10:09:56.427414Z","iopub.status.idle":"2024-11-06T10:09:58.245868Z","shell.execute_reply.started":"2024-11-06T10:09:56.427366Z","shell.execute_reply":"2024-11-06T10:09:58.244790Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"embed_model = HuggingFaceEmbedding(\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n    #device=device\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T10:09:59.933777Z","iopub.execute_input":"2024-11-06T10:09:59.934690Z","iopub.status.idle":"2024-11-06T10:10:02.176597Z","shell.execute_reply.started":"2024-11-06T10:09:59.934648Z","shell.execute_reply":"2024-11-06T10:10:02.175523Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the system prompt and query wrapper prompt\nsystem_prompt = \"\"\"\nYou are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\n\"\"\"\nquery_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n\n# Configure BitsAndBytes for 8-bit quantization to save memory\nbnb_config = BitsAndBytesConfig(load_in_8bit=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T10:10:05.478210Z","iopub.execute_input":"2024-11-06T10:10:05.479067Z","iopub.status.idle":"2024-11-06T10:10:05.483793Z","shell.execute_reply.started":"2024-11-06T10:10:05.479024Z","shell.execute_reply":"2024-11-06T10:10:05.482802Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import subprocess\nfrom kaggle_secrets import UserSecretsClient\n\n# Get the secret API token\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HUggingFaceLoginRag\")\n\n# Use the token to login via subprocess\nresult = subprocess.run(\n    ['huggingface-cli', 'login', '--token', hf_token],\n    text=True,\n    capture_output=True\n)\n\n# Check result\nif result.returncode == 0:\n    print(\"Logged in to Hugging Face successfully.\")\nelse:\n    print(\"Failed to log in:\", result.stderr)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T10:10:09.301346Z","iopub.execute_input":"2024-11-06T10:10:09.302279Z","iopub.status.idle":"2024-11-06T10:10:09.997591Z","shell.execute_reply.started":"2024-11-06T10:10:09.302235Z","shell.execute_reply":"2024-11-06T10:10:09.996598Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Logged in to Hugging Face successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"llm = HuggingFaceLLM(\n    context_window=4096,\n    max_new_tokens=256,\n    generate_kwargs={\"temperature\": 0.1, \"do_sample\": False},\n    system_prompt=system_prompt,\n    query_wrapper_prompt=query_wrapper_prompt,\n    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n    device_map=\"auto\"\n    # Additional parameters might be needed here depending on the actual class definition\n    #stopping_ids=[50278, 50279, 50277, 1, 0],\n    #tokenizer_kwargs={\"max_length\": 4096},\n    # uncomment this if using CUDA to reduce memory usage\n    # model_kwargs={\"torch_dtype\": torch.float16}\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T10:10:15.016728Z","iopub.execute_input":"2024-11-06T10:10:15.017708Z","iopub.status.idle":"2024-11-06T10:11:18.023738Z","shell.execute_reply.started":"2024-11-06T10:10:15.017663Z","shell.execute_reply":"2024-11-06T10:11:18.022806Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab294c1150d14fa1a318cae656c830a4"}},"metadata":{}}]},{"cell_type":"code","source":"from llama_index.core import Settings\n\nSettings.llm = llm\n\nSettings.chunk_size = 1024","metadata":{"execution":{"iopub.status.busy":"2024-11-06T10:11:37.831128Z","iopub.execute_input":"2024-11-06T10:11:37.831546Z","iopub.status.idle":"2024-11-06T10:11:37.836362Z","shell.execute_reply.started":"2024-11-06T10:11:37.831508Z","shell.execute_reply":"2024-11-06T10:11:37.835340Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"index = VectorStoreIndex.from_documents(\n    documents,\n    llm=Settings.llm,\n    embed_model=embed_model\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T10:11:39.506619Z","iopub.execute_input":"2024-11-06T10:11:39.507011Z","iopub.status.idle":"2024-11-06T10:11:41.501964Z","shell.execute_reply.started":"2024-11-06T10:11:39.506973Z","shell.execute_reply":"2024-11-06T10:11:41.501216Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42e397b5896347b4838a5946991924d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7246922f7454adc8d9ecd34a2143fb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b184426f8a14443488c9570df83c9011"}},"metadata":{}}]},{"cell_type":"code","source":"query_engine = index.as_query_engine()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T10:11:43.315804Z","iopub.execute_input":"2024-11-06T10:11:43.316731Z","iopub.status.idle":"2024-11-06T10:11:43.321265Z","shell.execute_reply.started":"2024-11-06T10:11:43.316685Z","shell.execute_reply":"2024-11-06T10:11:43.320205Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"response = query_engine.query(\"What is collaborative filtering?\")","metadata":{"execution":{"iopub.status.busy":"2024-11-06T10:11:45.965234Z","iopub.execute_input":"2024-11-06T10:11:45.966072Z","iopub.status.idle":"2024-11-06T10:12:18.712849Z","shell.execute_reply.started":"2024-11-06T10:11:45.966013Z","shell.execute_reply":"2024-11-06T10:12:18.711933Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a92891a78274df7ba9853a4aafadf8f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(response)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T10:12:47.124916Z","iopub.execute_input":"2024-11-06T10:12:47.125728Z","iopub.status.idle":"2024-11-06T10:12:47.130443Z","shell.execute_reply.started":"2024-11-06T10:12:47.125685Z","shell.execute_reply":"2024-11-06T10:12:47.129509Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Collaborative filtering is a technique used in recommendation systems to make personalized recommendations to users based on the preferences of other users with similar tastes and behavior. It works by analyzing the patterns of user behavior and item preferences to identify commonalities and make recommendations that are likely to be of interest to the user. Collaborative filtering can be further divided into two types: 1) neighborhood-based collaborative filtering, and 2) matrix factorization-based collaborative filtering. In neighborhood-based collaborative filtering, the system identifies users with similar preferences to the active user and uses their ratings to make recommendations. In matrix factorization-based collaborative filtering, the system represents the user-item matrix as a product of two lower-dimensional matrices, and uses these matrices to make recommendations. Both techniques have their advantages and disadvantages, and the choice of technique depends on the specific use case and the characteristics of the data.\n","output_type":"stream"}]},{"cell_type":"code","source":"response = query_engine.query(\"What is auto encoder and how it helps in colaborative filtering?\")\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T10:12:55.815672Z","iopub.execute_input":"2024-11-06T10:12:55.816626Z","iopub.status.idle":"2024-11-06T10:13:38.147052Z","shell.execute_reply.started":"2024-11-06T10:12:55.816585Z","shell.execute_reply":"2024-11-06T10:13:38.146015Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d83d281c67a947fd89a015341c293b0a"}},"metadata":{}},{"name":"stdout","text":"Autoencoders are neural networks that are trained to reconstruct their inputs. In the context of collaborative filtering, an autoencoder can be used to learn a compact and efficient representation of a user's preferences. The idea is to use the autoencoder to map the user's rating vector to a lower-dimensional latent space, where the latent space captures the user's preferences. This can help in collaborative filtering by allowing the system to make predictions about a user's ratings for items they have not rated before. The autoencoder can also be used to identify the most important features in the user's rating vector, which can help in personalizing the recommendations.\n\nIn the paper \"Autoencoders Meet Collaborative Filtering\" by Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie, the authors propose a novel autoencoder-based framework for collaborative filtering called AutoRec. The AutoRec model uses an item-based autoencoder to learn a compact and efficient representation of a user's preferences, and then uses this representation to make predictions about the user's ratings for items they have\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}